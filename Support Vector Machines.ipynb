{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4503f39",
   "metadata": {},
   "source": [
    "# Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3079413",
   "metadata": {},
   "source": [
    "The decision boundary of an SVM classifier not only separates the two classes but also stays as far away as possible from the closest training instances. It is like fitting the widest possible street between the classes. This is called *large margin classification*. \n",
    "\n",
    "Adding more training instances off the street do not affect the decision boundary at all, it is fully determined by the instances located on the edge of the street. These instances are called the *support vectors*.\n",
    "\n",
    "SVMs are sensitive to feature scales, so the features must be scaled using the **StandardScaler** before the model is fit on the training data.\n",
    "\n",
    "SVMs are good for classification on small to medium sized datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6f60d",
   "metadata": {},
   "source": [
    "### Soft Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0bb3b1",
   "metadata": {},
   "source": [
    "If we strictly impose that all instances must be off the street this is called *hard margin classification*. There are two main issues with hard margin classification.\n",
    "\n",
    "1. It only works if the data is linearly separable.\n",
    "2. It is sensitive to outliers and will not generalize well.\n",
    "\n",
    "To avoid these issues, we use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the *margin violations*. This is called *soft margin classification*. \n",
    "\n",
    "When creating an SVM model using Scikit-Learn, we can specify the hyperparameter **C**. If we set it to a lowe value, we end up with a model that is more soft margin than hard margin and vice versa. If an SVM model is overfitting, we can try regularizing it by reducing C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf47ea",
   "metadata": {},
   "source": [
    "The following code loads the iris dataset, scales the features, and then trains a linear SVM model to detect *Iris virginica* flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de411156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "331ed7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e2ed26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d9d951",
   "metadata": {},
   "source": [
    "Unlike Logistic Regression, SVM classifiers do not output probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf339790",
   "metadata": {},
   "source": [
    "Instead of using the **LinearSVC** class, we could use the SVC class with a linear kernel. When creating the SVC model, we would write **SVC(kernel=\"linear\", C=1)**. Or we could use the **SGDClassifier** class with **SGDClassifier(loss=\"hinge\", alpha=1/(m * C))**. This applies regular Stochastic Gradient Descent to train a linear SVM classifier. It does not converge as fast as the **LinearSVC** class, but it can be used for out-of-core training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0610af",
   "metadata": {},
   "source": [
    "It is important to set the loss hyperparameter to \"hinge\", as it is not the default value. For better performance, we should also set the **dual** hyperparameter to **False**, unless there are more features than training instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4415c2",
   "metadata": {},
   "source": [
    "# Nonlinear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06925448",
   "metadata": {},
   "source": [
    "One approach to handling nonlinear datasets is to add polynomial features which may result in a linearly separable daatset.\n",
    "\n",
    "The code below implements the above idea on the moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b968e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a889d4",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70703bf",
   "metadata": {},
   "source": [
    "Adding polynomial features is a good method but fails for data that is too complex since we cannot create a very degree polynomial because it may lead to feature explosion.\n",
    "\n",
    "SVMs have a workaround this with the *kernel trick*. It gets the exact same result as polynomial features except it doesn't actually add any features and only does the math using these extra features in the backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78b1e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd361a79",
   "metadata": {},
   "source": [
    "The hyperparameter **coef0** controls how much the model is influenced by high-degree polynomials versus low-degree polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc098f",
   "metadata": {},
   "source": [
    "### Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a48c81",
   "metadata": {},
   "source": [
    "Another technique to tackle nonlinear problems is to add features computed using a *similarity function*, which measures how much an instance resembles a particular *landmark*. For example, let's assume a 1D dataset and add two landmarks to it at $x_{1} = -2$ and $x_{1} = 1$. Defining the similarity function to be the Gaussian *Radial Basis Function* (RBF) with $\\gamma = 0.3$, the RBF equation is:\n",
    "\n",
    "$\\phi_{\\gamma} (x, l) = exp(- \\gamma || x - l ||^{2})$\n",
    "\n",
    "This is a bell-shaped function varying from 0 to 1. Now we are ready to compute the new features. Assume there is an instance $x_{1} = -1$, it is located at a distance of 1 from the first landmark and 2 from the second landmark. Therefore, its new features are $x_{2} = exp(-0.3 * 1^{2}) = 0.74$ and $x_{3} = exp(-0.3 * 2^{2}) = 0.30$.\n",
    "\n",
    "As for how to select landmarks, the simplest approach would be to create a landmark at the location of each and every instance in the dataset. Doing that creates many dimensions and increases the chance that the transformed training set is linearly separable. The downside is that a training set with *m* instances and *n* features gets converted to a training set with *m* instances and *m* features. If the training set is very large, we end up with an equally large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09153e7e",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534c34b",
   "metadata": {},
   "source": [
    "Just like polynomial features, the similarity features method also causes a feature explosion. Once again, SVM's kernel trick comes clutch and doesn't cause a feature explosion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53265141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b43813",
   "metadata": {},
   "source": [
    "This classifier has the hyperparemters **gamma** and **C**. Increasing **gamma** makes the bell-shaped curve narrower. As a result, each instance's range of influence is smaller, the decision boundary ends up being more irregular, wiggling around individual instances. Conversely, a small **gamma** makes the bell-shaped curve wider, instances have a large range of influence and the decision boundary ends up smoother. So $\\gamma$ acts like a regularization hyperparaameter. If the model is overfitting, it should be reduced; if it is unerfitting, it should be increased.\n",
    "\n",
    "The rule of thumb is to test out a LinearSVM or linear kernel SVM first, especially when the training set is large. If not, a Gaussian RBF kernel is the next best option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9cc2e",
   "metadata": {},
   "source": [
    "*Comparison of Scikit-Learn classes for SVM Classification*\n",
    "\n",
    "| Class    | Time Complexity | Out-of-core support | Scaling required | Kernel trick |\n",
    "| ---      |     :---:       | :---:               | :---:            | ---:         |\n",
    "| Linear SVC | $O(m * n)$ | No | Yes | No |\n",
    "| SGDClassifier | $O(m * n)$ | Yes | Yes | No |\n",
    "| SVC | $O(m^{2} * n)$ to $O(m^{3} * n)$ | No | Yes | Yes|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e573d",
   "metadata": {},
   "source": [
    "# SVM Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ebec7",
   "metadata": {},
   "source": [
    "The SVM algorithm supports linear and nonlinear regression too. To use SVMs for regression instead of classification, we must reverse the objective. Instead of trying to fit the largest possible street between the two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible *on* the street while limiting margin violations. The width of the street is controlled by the hyperparameter $\\epsilon$. Adding more training instances within the margin does not affect the model's predictions thus the model is said to be *$\\epsilon$-insensitive*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c9e67d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ab18ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f30aa6",
   "metadata": {},
   "source": [
    "SVMs can also be used for outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a1505",
   "metadata": {},
   "source": [
    "# Under The Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e85695",
   "metadata": {},
   "source": [
    "### Decision Functions and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c73044",
   "metadata": {},
   "source": [
    "The linear SVM classifer predicts the class of a new instance by simply computing the decision function $w^{T}x + b$. If the result is positive the predicted class $\\hat{y}$ is the positive class (1), else the negative class (0).\n",
    "\n",
    "The decision function corresponding to a data with two features is a 2D plane. The decision boundary is the set of points where the decision function is equal to 0, it is the intersection of two planes which is a straight line. The two support vectors are where the decision function is equal to -1 and 1. Training a linear SVM classifier means finding the values of **w** and **b** that make this margin as wide as possible while avoiding margin violations (hard margin) or limiting them (soft margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d5e7a4",
   "metadata": {},
   "source": [
    "### Training Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54abb683",
   "metadata": {},
   "source": [
    "The slope of the decision function is equal to the norm of the weight vector, **$||w||$.** If we divide this slope by 2, the points where the decision function is equal to -1 and 1 are going to be twice as far away from the decision boundary. Dividing the slope by 2 will multiply the margin by 2. The smaller the weight vector **w**, the larger the margin.\n",
    "\n",
    "So we want to minimize $||w||$ to get a large margin. If we also want to avoid any margin violations (hard margin), then we need the decision function to be greater than 1 for all positive instances and less than -1 for all negative instances. If we define $t^{(i)} = -1$ for negative instances and $t^{(i)} = 1$ for all positive instances then we can express this constraint as $t^{(i)} (w^{T} x^{(i)} + b) \\geq 1$ for all instances.\n",
    "\n",
    "We can therefore express the hard margin linear SVM classifier objective as the constrained optimization problem:\n",
    "\n",
    "$minimize_{w, b} \\frac{1}{2} w^{T} w$\n",
    "\n",
    "subject to $t^{(i)} (w^{T} x^{(i)} + b) \\geq 1$ for all instances\n",
    "\n",
    "We are minimizing $\\frac{1}{2} w^{T} w$, which is equal to $\\frac{1}{2} ||w||^{2}$ rather than minimizing $||w||$ because it has a nice, simple derivative, while $||w||$ is not differentiable at $w=0$. Optimization algorithms work much better on differentiable functions.\n",
    "\n",
    "To get the soft margin objective, we need to introduce a *slack variable* $\\zeta^{i} \\geq 0$, for each instance, it measures how much an instance is allowed to violate the margin. We now have two conflicting objectives: make the slack variables as small as possible to reduce the margin violations and make $\\frac{1}{2} w^{T} w$ as small as possible to increase the margin. The **C** hyperparameter allows us to define the trade-off between these two objectives. This gives us the constrained optimization problem:\n",
    "\n",
    "$minimize_{w, b, \\zeta} \\frac{1}{2} w^{T} w + C \\sum_{i=1}^{m} \\zeta^{(i)}$\n",
    "\n",
    "subject to $t^{(i)} (w^{T} x^{(i)} + b) \\geq 1 - \\zeta^{(i)} and \\zeta^{(i)} \\geq 0$ for all instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c8cec3",
   "metadata": {},
   "source": [
    "### Quadratic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6991a68",
   "metadata": {},
   "source": [
    "The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. Such problems are called *Quadratic Programming* (QP) problems. Many solvers are available to solve QP problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a865b6",
   "metadata": {},
   "source": [
    "### The Dual Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6eeab",
   "metadata": {},
   "source": [
    "Given a constrained optimization problem, known as the *primal problem*, it is possible to express a different but closely related problem, called its *dual problem*. The solution to the dual problem typically gives a lower bound to the solution of the primal problem, but unders some conditions it can have the same solution as the primal problem. The SVM problem happens to meet these conditions (the objective function is convex, and the inequality constraints are continuously differentiable and convex functions), so we can choose to solve the primal problem or the dual problem; both will have the same solution. *The dual form of the linear SVM objective*:\n",
    "\n",
    "$minimize_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} x^{(i)T} x^{(j)} - \\sum_{i=1}^{m} \\alpha^{(i)}$\n",
    "\n",
    "subject to $\\alpha^{(i)} \\geq 0$ for all instances i equals 1 to m.\n",
    "\n",
    "Once we find the vector $\\hat{\\alpha}$ that minimizes this equation using a QP solver, we can use the below equations to compute $\\hat{w}$ and $\\hat{b}$ that minimize the primal problem. *From the dual solution to the primal solution*:\n",
    "\n",
    "$\\hat{w} = \\sum_{i=1}^{m} \\hat{\\alpha}^{(i)} t^{(i)} x^{(i)}$\n",
    "\n",
    "$\\hat{b} = \\frac{1}{n_{s}} \\sum_{i=1}^{m} (t^{(i)} - \\hat{w}^{T} x^{(i)})$\n",
    "\n",
    "The dual problem is faster to solve than the primal one when the number of training instances is smaller than the number of features. More importantly, the dual problem makes the kernel trick possible, while the primal does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae459371",
   "metadata": {},
   "source": [
    "### Kernelized SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6d651",
   "metadata": {},
   "source": [
    "Suppose we want to apply a second-degree transformation to a two-dimensional training set (such as the moons training set), then train a linear SVM classifier on the transformed training set. The second-degree polynomial mapping $\\phi$ is given by:\n",
    "\n",
    "$\\phi(x) = [\\phi(x_{1}), \\phi(x_{2})] = [x_{1}^{2}, \\sqrt{2} x_{1} x_{2}, x_{2}^{2}]$\n",
    "\n",
    "The transformed vector is 3D instead of 2D. Let's see what happens to a couple of 2D vectors, **a** and **b**, if we apply a second degree transformation and then compute the dot product of the tranformed vectors:\n",
    "\n",
    "$\\phi(a)^{T} \\phi(b) = (a_{1} b_{1} + a_{2} b_{2})^{2} = (a^{T}b)^{2}$\n",
    "\n",
    "The dot product of the tranformed vectors is equal to the square of the dot product of the original vectors:\n",
    "\n",
    "$\\phi(a)^{T} \\phi(b) = (a^{T}b)^{2}$\n",
    "\n",
    "The key insight is that if we apply the transformation $\\phi$ to all training instances then the dual problem will contain the dot product $\\phi(x^{(i)})^{T} \\phi(x^{(j)})$. But if $\\phi$ is the second-degree polynomial transformation then we can replace this dot product of transformed vectors simply by $(x^{(i)T} x^{(j)})^{2}$. So we don't need to transform the training instances at all; just replace the dot product by its square. \n",
    "\n",
    "The function $K(a, b) = (a^{T}b)^{2}$ is a second-degree polynomial kernel. In ML, a *kernel* is a function capable of computing the dot product based only on the original vectors, without having to know about the transformation $\\phi$. *Common kernels*:\n",
    "\n",
    "Linear: $K(a, b) = a^{T}b$\n",
    "\n",
    "Polynomial: $K(a, b) = (\\gamma a^{T}b + r)^{d}$\n",
    "\n",
    "Gaussian RBF: $K(a, b) = exp(- \\gamma ||a - b||^{2})$\n",
    "\n",
    "Sigmoid: $K(a, b) = tanh(\\gamma a^{T}b + r)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b9e2e",
   "metadata": {},
   "source": [
    "There is still one loose end. The equations above show how to go from the dual solution to the primal solution in the case of a linear SVM classifier. But if we apply the kernel trick, we end up with equations that must include $\\phi(x^{(i)})$. In fact, $\\hat{w}$ must have the same number of dimensions as $\\phi(x^{(i)})$, which may be huge or even infinite, so we can't compute it. The work around is that we can plug the formula for $\\hat{w}$ into the decision function for a new instance $x^{(n)}$, we can get an equation with only dot products between input vectors. *Making predictions with a kernelized SVM*:\n",
    "\n",
    "$h_{\\hat{w}, \\hat{b}} (\\phi(x^{(n)})) + \\hat{b} = \\sum_{i=1}^{m} \\hat(\\alpha)^{(i)} t^{(i)} K(x^{(i)}, x^{(n)}) + \\hat{b}$.\n",
    "\n",
    "We can use the same trick to compute $\\hat{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367cb021",
   "metadata": {},
   "source": [
    "### Online Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f98ae5",
   "metadata": {},
   "source": [
    "For SVM classifiers, one method for implementing an online SVM classifier is to use Gradient Descent (**SGDClassifier**) to minimize the cost function:\n",
    "\n",
    "$J(w, b) = \\frac{1}{2} w^{T} w + C \\sum_{i=1}^{m} max(0, 1 - t^{(i)} (w^{T} x^{(i)} + b))$\n",
    "\n",
    "which is derived from the primal problem. Unfortunately, Gradient Descent converges much more slowly than the methods based on QP.\n",
    "\n",
    "The first sum in the cost function will push the model to have a small weight vector **w**, leading to a larger margin. The second sum computes the total of all margin violations. An instance's margin violation is equal to 0 if it is located off the street on the correct side, or else it is proportional to the distance of the correct side of the street. Minimizing this term ensures that the model makes the margin violations as small and as few as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd364c",
   "metadata": {},
   "source": [
    "### Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ca890",
   "metadata": {},
   "source": [
    "The function $max(0, 1-t)$ is called the *hinge loss* function. It is equal to 0 when $t \\geq 1$. Its derivative (slope) is equal to -1 if $t \\leq 1$ and 0 if $t \\geq 1$. It is not differentiable at $t=1$, but just like for Lasso Regression, we can still use Gradient Descent using any *subderivative* at $t=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a70b6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
